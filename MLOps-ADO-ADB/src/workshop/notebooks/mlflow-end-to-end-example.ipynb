{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f775ddb6-7daf-4f9f-99fa-7fc024351b47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Training machine learning models on tabular data: an end-to-end example\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "- Import data from your local machine into the Databricks File System (DBFS)\n",
    "- Visualize the data using Seaborn and matplotlib\n",
    "- Run a parallel hyperparameter sweep to train machine learning models on the dataset\n",
    "- Explore the results of the hyperparameter sweep with MLflow\n",
    "- Register the best performing model in MLflow\n",
    "- Apply the registered model to another dataset using a Spark UDF\n",
    "- Set up model serving for low-latency requests\n",
    "\n",
    "In this example, you build a model to predict the quality of Portugese \"Vinho Verde\" wine based on the wine's physicochemical properties. \n",
    "\n",
    "The example uses a dataset from the UCI Machine Learning Repository, presented in [*Modeling wine preferences by data mining from physicochemical properties*](https://www.sciencedirect.com/science/article/pii/S0167923609001377?via%3Dihub) [Cortez et al., 2009].\n",
    "\n",
    "## Requirements\n",
    "This notebook requires Databricks Runtime for Machine Learning.  \n",
    "If you are using Databricks Runtime 7.3 LTS ML or below, you must update the CloudPickle library. To do that, uncomment and run the `%pip install` command in Cmd 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eab982e-0862-4138-b473-53bfc84bcf48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This command is only required if you are using a cluster running DBR 7.3 LTS ML or below. \n",
    "#%pip install --upgrade cloudpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "366dccef-47e4-466b-af76-53e29ad99472",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import data\n",
    "  \n",
    "In this section, you download a dataset from the web and upload it to Databricks File System (DBFS).\n",
    "\n",
    "1. Navigate to https://archive.ics.uci.edu/dataset/186/wine+quality and download the dataset to your local machine. The download contains two .csv files, `winequality-red.csv` and `winequality-white.csv`.\n",
    "\n",
    "1. From this Databricks notebook, select **File > Upload data to DBFS...**, and drag these files to the drag-and-drop target to upload them to the Databricks File System (DBFS). \n",
    "\n",
    "    **Note**: if you don't have the **File > Upload data to DBFS...** option, you can load the dataset from the Databricks example datasets. Uncomment and run the last two lines in the following cell.\n",
    "\n",
    "1. Click **Next**. Auto-generated code to load the data appears. Under **Access Files from Notebooks**, select the **pandas** tab. Click **Copy** to copy the example code, and then click **Done**. \n",
    "\n",
    "1. Create a new cell, then paste in the sample code. It will look similar to the code shown in the following cell. Make these changes:\n",
    "  - Pass `sep=';'` to `pd.read_csv`\n",
    "  - Change the variable names from `df1` and `df2` to `white_wine` and `red_wine`, as shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fff57c10-e07e-47f2-b231-7ae3556ce157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# white_wine = pd.read_csv(\"/dbfs/FileStore/shared_uploads/nick.switanek@microsoft.com/winequality_red.csv\", sep=';')\n",
    "# red_wine = pd.read_csv(\"/dbfs/FileStore/shared_uploads/nick.switanek@microsoft.com/winequality_white.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e7323b-decf-43f0-ab1b-c3b5a5e0159e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you have the File > Upload Data menu option, follow the instructions in the previous cell to upload the data from your local machine.\n",
    "# The generated code, including the required edits described in the previous cell, is shown here for reference.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# In the following lines, replace <username> with your username.\n",
    "# white_wine = pd.read_csv(\"/dbfs/FileStore/shared_uploads/<username>/winequality_white.csv\", sep=';')\n",
    "# red_wine = pd.read_csv(\"/dbfs/FileStore/shared_uploads/<username>/winequality_red.csv\", sep=';')\n",
    "\n",
    "# If you do not have the File > Upload Data menu option, uncomment and run these lines to load the dataset.\n",
    "\n",
    "white_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-white.csv\", sep=\";\")\n",
    "red_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-red.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08eaf617-f2fd-4ea1-a054-633bd470b7f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Merge the two DataFrames into a single dataset, with a new binary feature \"is_red\" that indicates whether the wine is red or white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e989d5-949f-4bec-9122-060b995a2f11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "red_wine['is_red'] = 1\n",
    "white_wine['is_red'] = 0\n",
    "\n",
    "data = pd.concat([red_wine, white_wine], axis=0)\n",
    "\n",
    "# Remove spaces from column names\n",
    "data.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5ae58a-d4d4-4006-9a8d-bc7ae8131c12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ce0fcc0-6131-4db8-8497-5e525b7252c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Visualize data\n",
    "\n",
    "Before training a model, explore the dataset using Seaborn and Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31cf51f6-4bd1-4aa5-ac59-9deede4ccf86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Plot a histogram of the dependent variable, quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f38e522-42e9-4673-8bfa-43a5fc28ee6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(data.quality, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce08fec-03ea-4d8a-846e-b602ea950be3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Looks like quality scores are normally distributed between 3 and 9. \n",
    "\n",
    "Define a wine as high quality if it has quality >= 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "501a185a-3125-4ac4-83c1-61d28b529eaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "high_quality = (data.quality >= 7).astype(int)\n",
    "data.quality = high_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7b8d9dc-4408-4a84-9a5d-3d187960d9fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Box plots are useful in noticing correlations between features and a binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee658d2-fb77-451b-b119-f680eb8f345b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dims = (3, 4)\n",
    "\n",
    "f, axes = plt.subplots(dims[0], dims[1], figsize=(25, 15))\n",
    "axis_i, axis_j = 0, 0\n",
    "for col in data.columns:\n",
    "  if col == 'is_red' or col == 'quality':\n",
    "    continue # Box plots cannot be used on indicator variables\n",
    "  sns.boxplot(x=high_quality, y=data[col], ax=axes[axis_i, axis_j])\n",
    "  axis_j += 1\n",
    "  if axis_j == dims[1]:\n",
    "    axis_i += 1\n",
    "    axis_j = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c379687d-03da-41aa-bdf5-953db5ec9534",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the above box plots, a few variables stand out as good univariate predictors of quality. \n",
    "\n",
    "- In the alcohol box plot, the median alcohol content of high quality wines is greater than even the 75th quantile of low quality wines. High alcohol content is correlated with quality.\n",
    "- In the density box plot, low quality wines have a greater density than high quality wines. Density is inversely correlated with quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27b6967c-5a25-4fe6-894c-4c07a63014d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Preprocess data\n",
    "Prior to training a model, check for missing values and split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "601c2717-c8a5-4411-ba78-e033a9bcdd83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aff3310f-e556-44b6-8f7d-e547682e7677",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "379abb9a-bd5b-46b9-8975-ce91858eba39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prepare dataset for training baseline model\n",
    "Split the input data into 3 sets:\n",
    "- Train (60% of the dataset used to train the model)\n",
    "- Validation (20% of the dataset used to tune the hyperparameters)\n",
    "- Test (20% of the dataset used to report the true performance of the model on an unseen dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e70c1c07-0048-4bb8-a0c3-17a7895bb4b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop([\"quality\"], axis=1)\n",
    "y = data.quality\n",
    "\n",
    "# Split out the training data\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.6, random_state=123)\n",
    "\n",
    "# Split the remaining data equally into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224d0ddf-3925-4849-bc20-564175c212e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Build a baseline model\n",
    "This task seems well suited to a random forest classifier, since the output is binary and there may be interactions between multiple variables.\n",
    "\n",
    "The following code builds a simple classifier using scikit-learn. It uses MLflow to keep track of the model accuracy, and to save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46af6002-8063-4ada-b7e5-6ce56c22f60a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "import cloudpickle\n",
    "import time\n",
    "\n",
    "# The predict method of sklearn's RandomForestClassifier returns a binary classification (0 or 1). \n",
    "# The following code creates a wrapper function, SklearnModelWrapper, that uses \n",
    "# the predict_proba method to return the probability that the observation belongs to each class. \n",
    "\n",
    "class SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    \n",
    "  def predict(self, context, model_input):\n",
    "    return self.model.predict_proba(model_input)[:,1]\n",
    "\n",
    "# mlflow.start_run creates a new MLflow run to track the performance of this model. \n",
    "# Within the context, you call mlflow.log_param to keep track of the parameters used, and\n",
    "# mlflow.log_metric to record metrics like accuracy.\n",
    "with mlflow.start_run(run_name='untuned_random_forest'):\n",
    "  n_estimators = 10\n",
    "  model = RandomForestClassifier(n_estimators=n_estimators, random_state=np.random.RandomState(123))\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # predict_proba returns [prob_negative, prob_positive], so slice the output with [:, 1]\n",
    "  predictions_test = model.predict_proba(X_test)[:,1]\n",
    "  auc_score = roc_auc_score(y_test, predictions_test)\n",
    "  mlflow.log_param('n_estimators', n_estimators)\n",
    "  # Use the area under the ROC curve as a metric.\n",
    "  mlflow.log_metric('auc', auc_score)\n",
    "  wrappedModel = SklearnModelWrapper(model)\n",
    "  # Log the model with a signature that defines the schema of the model's inputs and outputs. \n",
    "  # When the model is deployed, this signature will be used to validate inputs.\n",
    "  signature = infer_signature(X_train, wrappedModel.predict(None, X_train))\n",
    "  \n",
    "  # MLflow contains utilities to create a conda environment used to serve models.\n",
    "  # The necessary dependencies are added to a conda.yaml file which is logged along with the model.\n",
    "  conda_env =  _mlflow_conda_env(\n",
    "        additional_conda_deps=None,\n",
    "        additional_pip_deps=[\"cloudpickle=={}\".format(cloudpickle.__version__), \"scikit-learn=={}\".format(sklearn.__version__)],\n",
    "        additional_conda_channels=None,\n",
    "    )\n",
    "  mlflow.pyfunc.log_model(\"random_forest_model\", python_model=wrappedModel, conda_env=conda_env, signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "867b3e23-9147-4ca4-b2d2-f1103c471d53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Examine the learned feature importances output by the model as a sanity-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e174abe-774b-461e-b390-212e11d1c68d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns.tolist(), columns=['importance'])\n",
    "feature_importances.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a939b8-7232-464b-8370-2bc9e3a8c613",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As illustrated by the boxplots shown previously, both alcohol and density are important in predicting quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8981b9f-8833-4dc4-a1cb-b25f9f43a8c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You logged the Area Under the ROC Curve (AUC) to MLflow. Click **Experiment** at the upper right to display the Experiment Runs sidebar. \n",
    "\n",
    "The model achieved an AUC of 0.854.\n",
    "\n",
    "A random classifier would have an AUC of 0.5, and higher AUC values are better. For more information, see [Receiver Operating Characteristic Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d1df4d8-4b50-47d8-ad89-d3c80ce960ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Register the model in MLflow Model Registry\n",
    "\n",
    "By registering this model in Model Registry, you can easily reference the model from anywhere within Databricks.\n",
    "\n",
    "The following section shows how to do this programmatically, but you can also register a model using the UI. See \"[Create or register a model using the UI](https://docs.microsoft.com/azure/databricks/applications/machine-learning/manage-model-lifecycle/index#create-or-register-a-model-using-the-ui)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f7b7e14-b016-41c1-9852-aae0f6625e93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_id = mlflow.search_runs(filter_string='tags.mlflow.runName = \"untuned_random_forest\"').iloc[0].run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4774edbf-f3fb-495c-a638-ec63f275f885",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you see the error \"PERMISSION_DENIED: User does not have any permission level assigned to the registered model\", \n",
    "# the cause may be that a model already exists with the name \"wine_quality\". Try using a different name.\n",
    "model_name = \"wine_quality\"\n",
    "model_version = mlflow.register_model(f\"runs:/{run_id}/random_forest_model\", model_name)\n",
    "\n",
    "# Registering the model takes a few seconds, so add a small delay\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f16d91-13ad-4e28-866f-3a979eb6fde1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You should now see the model in the Models page. To display the Models page, click the Models icon in the left sidebar. \n",
    "\n",
    "Next, transition this model to production and load it into this notebook from Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb032bd6-7e9d-4430-8c13-895ebc189297",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "  name=model_name,\n",
    "  version=model_version.version,\n",
    "  stage=\"Production\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f284978e-9c9b-4cd2-8bdc-1393991c5701",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The Models page now shows the model version in stage \"Production\".\n",
    "\n",
    "You can now refer to the model using the path \"models:/wine_quality/production\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58b22211-62ea-4fdd-a6ff-c552a65fa6e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/production\")\n",
    "\n",
    "# Sanity-check: This should match the AUC logged by MLflow\n",
    "print(f'AUC: {roc_auc_score(y_test, model.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bc8e62b-0ee7-42bf-838a-45c020259814",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Experiment with a new model\n",
    "\n",
    "The random forest model performed well even without hyperparameter tuning.\n",
    "\n",
    "The following code uses the xgboost library to train a more accurate model. It runs a parallel hyperparameter sweep to train multiple\n",
    "models in parallel, using Hyperopt and SparkTrials. As before, the code tracks the performance of each parameter configuration with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f69756-98aa-4e0e-b8bf-0a7d84dbdb05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from math import exp\n",
    "import mlflow.xgboost\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "search_space = {\n",
    "  'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "  'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "  'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "  'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "  'objective': 'binary:logistic',\n",
    "  'seed': 123, # Set a seed for deterministic training\n",
    "}\n",
    "\n",
    "def train_model(params):\n",
    "  # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.\n",
    "  mlflow.xgboost.autolog()\n",
    "  with mlflow.start_run(nested=True):\n",
    "    train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "    validation = xgb.DMatrix(data=X_val, label=y_val)\n",
    "    # Pass in the validation set so xgb can track an evaluation metric. XGBoost terminates training when the evaluation metric\n",
    "    # is no longer improving.\n",
    "    booster = xgb.train(params=params, dtrain=train, num_boost_round=1000,\\\n",
    "                        evals=[(validation, \"validation\")], early_stopping_rounds=50)\n",
    "    validation_predictions = booster.predict(validation)\n",
    "    auc_score = roc_auc_score(y_val, validation_predictions)\n",
    "    mlflow.log_metric('auc', auc_score)\n",
    "\n",
    "    signature = infer_signature(X_train, booster.predict(train))\n",
    "    mlflow.xgboost.log_model(booster, \"model\", signature=signature)\n",
    "    \n",
    "    # Set the loss to -1*auc_score so fmin maximizes the auc_score\n",
    "    return {'status': STATUS_OK, 'loss': -1*auc_score, 'booster': booster.attributes()}\n",
    "\n",
    "# Greater parallelism will lead to speedups, but a less optimal hyperparameter sweep. \n",
    "# A reasonable value for parallelism is the square root of max_evals.\n",
    "spark_trials = SparkTrials(parallelism=10)\n",
    "\n",
    "# Run fmin within an MLflow run context so that each hyperparameter configuration is logged as a child run of a parent\n",
    "# run called \"xgboost_models\" .\n",
    "with mlflow.start_run(run_name='xgboost_models'):\n",
    "  best_params = fmin(\n",
    "    fn=train_model, \n",
    "    space=search_space, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=96,\n",
    "    trials=spark_trials,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "012e3c74-236f-4166-bb15-b129eb01181a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Use MLflow to view the results\n",
    "Open up the Experiment Runs sidebar to see the MLflow runs. Click on Date next to the down arrow to display a menu, and select 'auc' to display the runs sorted by the auc metric. The highest auc value is 0.90.\n",
    "\n",
    "MLflow tracks the parameters and performance metrics of each run. Click the External Link icon <img src=\"https://docs.databricks.com/_static/images/icons/external-link.png\"/> at the top of the Experiment Runs sidebar to navigate to the MLflow Runs Table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00e5ea89-0918-44b3-9fca-60f7f48fb6b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now investigate how the hyperparameter choice correlates with AUC. Click the \"+\" icon to expand the parent run, then select all runs except the parent, and click \"Compare\". Select the Parallel Coordinates Plot.\n",
    "\n",
    "The Parallel Coordinates Plot is useful in understanding the impact of parameters on a metric. You can drag the pink slider bar at the upper right corner of the plot to highlight a subset of AUC values and the corresponding parameter values. The plot below highlights the highest AUC values:\n",
    "\n",
    "<img src=\"https://docs.databricks.com/_static/images/mlflow/end-to-end-example/parallel-coordinates-plot.png\"/>\n",
    "\n",
    "Notice that all of the top performing runs have a low value for reg_lambda and learning_rate. \n",
    "\n",
    "You could run another hyperparameter sweep to explore even lower values for these parameters. For simplicity, that step is not included in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1752fa82-55e2-457c-b6d3-d545483b1eae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You used MLflow to log the model produced by each hyperparameter configuration. The following code finds the best performing run and saves the model to Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac2d08c5-5381-40e8-b8a2-a0737ca5f934",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_run = mlflow.search_runs(order_by=['metrics.auc DESC']).iloc[0]\n",
    "print(f'AUC of Best Run: {best_run[\"metrics.auc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c34b4cf8-35f1-48e8-bc64-51b6b4cf0f70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Update the production `wine_quality` model in MLflow Model Registry\n",
    "\n",
    "Earlier, you saved the baseline model to Model Registry with the name `wine_quality`. Now that you have a created a more accurate model, update `wine_quality`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7419396-4da9-4181-8a3d-613b80d3544b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_model_version = mlflow.register_model(f\"runs:/{best_run.run_id}/model\", model_name)\n",
    "\n",
    "# Registering the model takes a few seconds, so add a small delay\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faaceaa1-fb8c-4e72-80c6-b9c7c5815a18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Click **Models** in the left sidebar to see that the `wine_quality` model now has two versions. \n",
    "\n",
    "The following code promotes the new version to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49c2eaec-eaf5-4e38-ab58-1d029271e4b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Archive the old model version\n",
    "client.transition_model_version_stage(\n",
    "  name=model_name,\n",
    "  version=model_version.version,\n",
    "  stage=\"Archived\"\n",
    ")\n",
    "\n",
    "# Promote the new model version to Production\n",
    "client.transition_model_version_stage(\n",
    "  name=model_name,\n",
    "  version=new_model_version.version,\n",
    "  stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11553e11-faea-43ae-aac1-10545a252e64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Clients that call load_model now receive the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c7a24e2-9eb2-432b-af35-e3bf3dc03a46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code is the same as the last block of \"Building a Baseline Model\". No change is required for clients to get the new model!\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/production\")\n",
    "print(f'AUC: {roc_auc_score(y_test, model.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "383276bc-bdfe-416f-956d-c45773af7a00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The auc value on the test set for the new model is 0.90. You beat the baseline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2be6ef67-12ae-479a-8ba9-2a9ff90224b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Batch inference\n",
    "\n",
    "There are many scenarios where you might want to evaluate a model on a corpus of new data. For example, you may have a fresh batch of data, or may need to compare the performance of two models on the same corpus of data.\n",
    "\n",
    "The following code evaluates the model on data stored in a Delta table, using Spark to run the computation in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b42b701-ccf1-4bd0-a9dd-1e705bcdd795",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To simulate a new corpus of data, save the existing X_train data to a Delta table. \n",
    "# In the real world, this would be a new batch of data.\n",
    "spark_df = spark.createDataFrame(X_train)\n",
    "# Replace <username> with your username before running this cell.\n",
    "table_path = \"dbfs:/<username>/delta/wine_data\"\n",
    "# Delete the contents of this path in case this cell has already been run\n",
    "dbutils.fs.rm(table_path, True)\n",
    "spark_df.write.format(\"delta\").save(table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1be915c-f0ea-4003-acc3-a56941da3ca8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Load the model into a Spark UDF, so it can be applied to the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9a1e264-e441-46b5-b38c-b896f7d21b1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "apply_model_udf = mlflow.pyfunc.spark_udf(spark, f\"models:/{model_name}/production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6bd7794-a6d3-4f87-8b06-11cdef5ac607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the \"new data\" from Delta\n",
    "new_data = spark.read.format(\"delta\").load(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b09df20f-a6c7-40ba-a23d-cf158bc56cb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa22c7ec-d8c9-4cdd-a7f6-4ba366978cee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "# Apply the model to the new data\n",
    "udf_inputs = struct(*(X_train.columns.tolist()))\n",
    "\n",
    "new_data = new_data.withColumn(\n",
    "  \"prediction\",\n",
    "  apply_model_udf(udf_inputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3437534-cf5f-4acb-bfb4-23de89f88ddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Each row now has an associated prediction. Note that the xgboost function does not output probabilities by default, so the predictions are not limited to the range [0, 1].\n",
    "display(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "485cdc68-326a-4a17-ac48-5458413e8e8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Model serving\n",
    "\n",
    "To productionize the model for low latency predictions, use Databricks Model Serving ([AWS](https://docs.databricks.com/machine-learning/model-serving/index.html)|[Azure](https://docs.microsoft.com/azure/databricks/machine-learning/model-serving/index)) to deploy the model to an endpoint.\n",
    "\n",
    "The following code illustrates how to issue requests using a REST API to get predictions from the deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99b2067d-cd94-4ec7-9216-ed36c6253fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You need a Databricks token to issue requests to your model endpoint. You can generate a token from the User Settings page (click Settings in the left sidebar). Copy the token into the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06f073d5-9b77-41e1-8bcd-67f17e288d2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = \"<YOUR_TOKEN>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "305d8fd0-3e43-4100-8ad3-a6f21eb8dddf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Click **Models** in the left sidebar and navigate to the registered wine model. Click the serving tab, and then click **Enable Serving**.\n",
    "\n",
    "Then, under **Call The Model**, click the **Python** button to display a Python code snippet to issue requests. Copy the code into this notebook. It should look similar to the code in the next cell. \n",
    "\n",
    "You can use the token to make these requests from outside Databricks notebooks as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8962617-79a4-4962-9491-dd77dbb95dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace with code snippet from the model serving page\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def score_model(dataset: pd.DataFrame):\n",
    "  url = 'https://<DATABRICKS_URL>/model/wine_quality/Production/invocations'\n",
    "  headers = {'Authorization': f'Bearer {os.environ.get(\"DATABRICKS_TOKEN\")}'}\n",
    "  data_json = dataset.to_dict(orient='records')\n",
    "  response = requests.request(method='POST', headers=headers, url=url, json=data_json)\n",
    "  if response.status_code != 200:\n",
    "    raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "  return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb6ed38-6684-492c-8d01-ab2352103246",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The model predictions from the endpoint should agree with the results from locally evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2906da04-5ef6-4366-8ac4-bfa93ec21658",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model serving is designed for low-latency predictions on smaller batches of data\n",
    "num_predictions = 5\n",
    "served_predictions = score_model(X_test[:num_predictions])\n",
    "model_evaluations = model.predict(X_test[:num_predictions])\n",
    "# Compare the results from the deployed model and the trained model\n",
    "pd.DataFrame({\n",
    "  \"Model Prediction\": model_evaluations,\n",
    "  \"Served Model Prediction\": served_predictions,\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "mlflow-end-to-end-example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31f31ff4-9739-47a7-9610-94cc9522f8c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Experiment with a new model\n",
    "\n",
    "The random forest model performed well even without hyperparameter tuning.\n",
    "\n",
    "The following code uses the xgboost library to train a more accurate model. It runs a parallel hyperparameter sweep to train multiple\n",
    "models in parallel, using Hyperopt and SparkTrials. As before, the code tracks the performance of each parameter configuration with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "022fbb0a-62fe-4fb2-9eb0-ccc06af2e583",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from math import exp\n",
    "import mlflow.xgboost\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "search_space = {\n",
    "  'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "  'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "  'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "  'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "  'objective': 'binary:logistic',\n",
    "  'seed': 123, # Set a seed for deterministic training\n",
    "}\n",
    "\n",
    "def train_model(params):\n",
    "  # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.\n",
    "  mlflow.xgboost.autolog()\n",
    "  with mlflow.start_run(nested=True):\n",
    "    train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "    validation = xgb.DMatrix(data=X_val, label=y_val)\n",
    "    # Pass in the validation set so xgb can track an evaluation metric. XGBoost terminates training when the evaluation metric\n",
    "    # is no longer improving.\n",
    "    booster = xgb.train(params=params, dtrain=train, num_boost_round=1000,\\\n",
    "                        evals=[(validation, \"validation\")], early_stopping_rounds=50)\n",
    "    validation_predictions = booster.predict(validation)\n",
    "    auc_score = roc_auc_score(y_val, validation_predictions)\n",
    "    mlflow.log_metric('auc', auc_score)\n",
    "\n",
    "    signature = infer_signature(X_train, booster.predict(train))\n",
    "    mlflow.xgboost.log_model(booster, \"model\", signature=signature)\n",
    "    \n",
    "    # Set the loss to -1*auc_score so fmin maximizes the auc_score\n",
    "    return {'status': STATUS_OK, 'loss': -1*auc_score, 'booster': booster.attributes()}\n",
    "\n",
    "# Greater parallelism will lead to speedups, but a less optimal hyperparameter sweep. \n",
    "# A reasonable value for parallelism is the square root of max_evals.\n",
    "spark_trials = SparkTrials(parallelism=10)\n",
    "\n",
    "# Run fmin within an MLflow run context so that each hyperparameter configuration is logged as a child run of a parent\n",
    "# run called \"xgboost_models\" .\n",
    "with mlflow.start_run(run_name='xgboost_models'):\n",
    "  best_params = fmin(\n",
    "    fn=train_model, \n",
    "    space=search_space, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=96,\n",
    "    trials=spark_trials,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2991436a-2591-4436-89b2-65420c52f0e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Use MLflow to view the results\n",
    "Open up the Experiment Runs sidebar to see the MLflow runs. Click on Date next to the down arrow to display a menu, and select 'auc' to display the runs sorted by the auc metric. The highest auc value is 0.90.\n",
    "\n",
    "MLflow tracks the parameters and performance metrics of each run. Click the External Link icon <img src=\"https://docs.databricks.com/_static/images/icons/external-link.png\"/> at the top of the Experiment Runs sidebar to navigate to the MLflow Runs Table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "325e2c3c-4164-4f73-9022-0b954a7ce4b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now investigate how the hyperparameter choice correlates with AUC. Click the \"+\" icon to expand the parent run, then select all runs except the parent, and click \"Compare\". Select the Parallel Coordinates Plot.\n",
    "\n",
    "The Parallel Coordinates Plot is useful in understanding the impact of parameters on a metric. You can drag the pink slider bar at the upper right corner of the plot to highlight a subset of AUC values and the corresponding parameter values. The plot below highlights the highest AUC values:\n",
    "\n",
    "<img src=\"https://docs.databricks.com/_static/images/mlflow/end-to-end-example/parallel-coordinates-plot.png\"/>\n",
    "\n",
    "Notice that all of the top performing runs have a low value for reg_lambda and learning_rate. \n",
    "\n",
    "You could run another hyperparameter sweep to explore even lower values for these parameters. For simplicity, that step is not included in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "773d95c7-4df6-47e3-b897-26e01a0cf1a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You used MLflow to log the model produced by each hyperparameter configuration. The following code finds the best performing run and saves the model to Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ead8b6cb-bd7e-447a-8271-c283b0dcba63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_run = mlflow.search_runs(order_by=['metrics.auc DESC']).iloc[0]\n",
    "print(f'AUC of Best Run: {best_run[\"metrics.auc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dbc4e92-20de-44f5-96b8-506c413d0268",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Update the production `wine_quality` model in MLflow Model Registry\n",
    "\n",
    "Earlier, you saved the baseline model to Model Registry with the name `wine_quality`. Now that you have a created a more accurate model, update `wine_quality`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b67f77-31cd-405e-9d28-f3519b517b3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_model_version = mlflow.register_model(f\"runs:/{best_run.run_id}/model\", model_name)\n",
    "\n",
    "# Registering the model takes a few seconds, so add a small delay\n",
    "time.sleep(15)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "part_4_new_training_code",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

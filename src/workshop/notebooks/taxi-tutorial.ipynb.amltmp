{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Build a regression model with Open Datasets\n",
        "\n",
        "In this tutorial, you leverage the convenience of Azure Open Datasets to create a regression model to predict NYC taxi fare prices. Easily download publicly available taxi, holiday and weather data to create a dataset that can train a regression model using sklearn."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.opendatasets import NycTlcGreen\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data\n",
        "Begin by downloading the NYC Taxi dataset from Azure Open Datasets. In non-Spark environments, Open Datasets only allows one month of data at a time with certain classes to avoid MemoryError with large datasets. To download 1 year of taxi data, we will fetch 2000 random samples from each month.\n",
        "\n",
        "Note: Open Datasets has mirroring classes for working in Spark where data size and memory are not a concern."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.strptime(\"1/1/2016\",\"%m/%d/%Y\")\n",
        "end = datetime.strptime(\"1/31/2016\",\"%m/%d/%Y\")\n",
        "\n",
        "green_taxi_df = pd.concat([NycTlcGreen(start + relativedelta(months=x), end + relativedelta(months=x)) \\\n",
        "        .to_pandas_dataframe().sample(2000) for x in range(12)])\n",
        "green_taxi_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the initial data is loaded, define a function to create various time-based features from the pickup datetime field. This will create new fields for the month number, day of month, day of week, and hour of day. From those, we calculate the sin and cosine transformations to capture the cyclical nature of the variable which will allow the model to factor in time-based seasonality. This function also adds a static feature for the country code to join the holiday data. Use the apply() function on the dataframe to interatively apply this function to each row in the dataframe."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def build_time_features(vector):\n",
        "    pickup_datetime = vector[0]\n",
        "    month_num = pickup_datetime.month\n",
        "    day_of_month = pickup_datetime.day\n",
        "    day_of_week = pickup_datetime.weekday()\n",
        "    hour_of_day = pickup_datetime.hour\n",
        "    country_code = \"US\"\n",
        "    hr_sin = np.sin(hour_of_day*(2.*np.pi/24))\n",
        "    hr_cos = np.cos(hour_of_day*(2.*np.pi/24))\n",
        "    dy_sin = np.sin(day_of_week*(2.*np.pi/7))\n",
        "    dy_cos = np.cos(day_of_week*(2.*np.pi/7))\n",
        "    \n",
        "    return pd.Series((month_num, day_of_month, day_of_week, hour_of_day, country_code, hr_sin, hr_cos, dy_sin, dy_cos))\n",
        "\n",
        "green_taxi_df[[\"month_num\", \"day_of_month\",\"day_of_week\", \"hour_of_day\", \"country_code\", \"hr_sin\", \"hr_cos\", \"dy_sin\", \"dy_cos\"]] = green_taxi_df[[\"lpepPickupDatetime\"]].apply(build_time_features, axis=1)\n",
        "green_taxi_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove some of the columns that you won't need for modeling or additional feature building. Rename the time field for pickup time, and additionally convert the time to midnight using `pandas.Series.dt.normalize`. This is done to all time features so that the datetime column can be later used as a key when joining datasets together at a daily level of granularity."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = [\"lpepDropoffDatetime\", \"puLocationId\", \"doLocationId\", \"extra\", \"mtaTax\",\n",
        "                     \"improvementSurcharge\", \"tollsAmount\", \"ehailFee\", \"tripType\", \"rateCodeID\", \n",
        "                     \"storeAndFwdFlag\", \"paymentType\", \"fareAmount\", \"tipAmount\"]\n",
        "\n",
        "green_taxi_df.drop(columns_to_remove, axis=1, inplace=True)\n",
        "\n",
        "green_taxi_df[\"datetime\"] = green_taxi_df[\"lpepPickupDatetime\"].dt.normalize()\n",
        "green_taxi_df.head(5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enrich with Holiday Data\n",
        "\n",
        "Now that the taxi data is downloaded and roughly prepared, add in holiday data as additional features. Holiday-specific features will assist model accuracy, as major holidays are times where taxi demand increases dramatically and supply becomes limited. The holiday dataset is relatively small, so fetch the full set by using the `PublicHolidays` class constructor with no parameters for filtering. Preview the data to check the format."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.opendatasets import PublicHolidays\n",
        "\n",
        "# call default constructor to download full dataset\n",
        "holidays_df = PublicHolidays().to_pandas_dataframe()\n",
        "holidays_df.head(5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename the `countryRegionCode` and `date` columns to match the respective field names from the taxi data, and also normalize the time so it can be used as a key. Next, join the holiday data with the taxi data by performing a left-join using the Pandas `merge()` function. This will preserve all records from `green_taxi_df`, but add in holiday data where it exists for the corresponding `datetime` and `country_code`, which in this case is always `\\\"US\\\"`. Preview the data to verify that they were merged correctly."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "holidays_df = holidays_df.rename(columns={\"countryRegionCode\": \"country_code\"})\n",
        "holidays_df[\"datetime\"] = holidays_df[\"date\"].dt.normalize()\n",
        "\n",
        "holidays_df.drop([\"countryOrRegion\", \"holidayName\", \"date\"], axis=1, inplace=True)\n",
        "\n",
        "taxi_holidays_df = pd.merge(green_taxi_df, holidays_df, how=\"left\", on=[\"datetime\", \"country_code\"])\n",
        "taxi_holidays_df[taxi_holidays_df[\"normalizeHolidayName\"].notnull()]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enrich with weather data\n",
        "\n",
        "Now NOAA surface weather data can be appended to the taxi and holiday data. Use a similar approach to fetch the weather data by downloading one month at a time iteratively. Additionally, specify the `cols` parameter with an array of strings to filter the columns to download. This is a very large dataset containing weather surface data from all over the world, so before appending each month, filter the lat/long fields to near NYC using the `query()` function on the dataframe. This will ensure the `weather_df` doesn't get too large."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.opendatasets import NoaaIsdWeather\n",
        "start = datetime.strptime(\"1/1/2016\",\"%m/%d/%Y\")\n",
        "end = datetime.strptime(\"1/31/2016\",\"%m/%d/%Y\")\n",
        "\n",
        "weather_df = pd.concat([NoaaIsdWeather(cols=[\"temperature\", \"precipTime\", \"precipDepth\"], start_date=start + relativedelta(months=x), end_date=end + relativedelta(months=x))\\\n",
        "        .to_pandas_dataframe().query(\"latitude>=40.53 and latitude<=40.88 and longitude>=-74.09 and longitude<=-73.72 and temperature==temperature\") for x in range(12)])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again call `pandas.Series.dt.normalize` on the `datetime` field in the weather data so it matches the time key in `taxi_holidays_df`.\n",
        "\n",
        "\n",
        "Next group the weather data to have daily aggregated weather values. Define a dict `aggregations` to define how to aggregate each field at a daily level. For`temperature` take the mean and for `precipTime` and `precipDepth` take the daily maximum. Use the `groupby()` function along with the aggregations to group the data. Preview the data to ensure there is one record per day."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df[\"datetime\"] = weather_df[\"datetime\"].dt.normalize()\n",
        "\n",
        "# group by datetime\n",
        "aggregations = {\"precipTime\": \"max\", \"temperature\": \"mean\", \"precipDepth\": \"max\"}\n",
        "weather_df_grouped = weather_df.groupby(\"datetime\").agg(aggregations)\n",
        "weather_df_grouped.head(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The examples in this tutorial merge data using Pandas functions and custom aggregations, but the Open Datasets SDK has classes designed to easily merge and enrich data sets. See the [notebook](https://github.com/Azure/OpenDatasetsNotebooks/blob/master/tutorials/data-join/04-nyc-taxi-join-weather-in-pandas.ipynb) for code examples of these design patterns."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleanse data\n",
        "\n",
        "Merge the existing taxi and holiday data with the new weather data. This time `datetime` is the only key, and again perform a left-join of the data. Run the `describe()` function on the new dataframe to see summary statistics for each field."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_holidays_weather_df = pd.merge(taxi_holidays_df, weather_df_grouped, how=\"left\", on=[\"datetime\"])\n",
        "taxi_holidays_weather_df.describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the summary statistics, you see that there are several fields that have outliers or values that will reduce model accuracy. First filter the lat/long fields to be within the same bounds you used for filtering weather data. The `tripDistance` field has some bad data, because the minimum value is negative. The `passengerCount` field has bad data as well, with the max value being 210 passengers. Lastly, the `totalAmount` field has negative values, which don't make sense in the context of our model.\n",
        "\n",
        "Filter out these anomolies using query functions, and then remove the last few columns unnecesary for training.\n",
        "\n",
        "Note: since a random sample of 2000 was taken for each month of the taxi data, the statistics may vary each time this is ran."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = taxi_holidays_weather_df.query(\"pickupLatitude>=40.53 and pickupLatitude<=40.88 and \\\n",
        "                                           pickupLongitude>=-74.09 and pickupLongitude<=-73.72 and \\\n",
        "                                           tripDistance>0 and tripDistance<75 and \\\n",
        "                                           passengerCount>0 and passengerCount<100 and \\\n",
        "                                           totalAmount>0\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call `describe()` again on the data to ensure cleansing worked as expected. The final data is prepared and cleansed, consisting of taxi, holiday, and weather data, and is ready to use for machine learning model training."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a model\n",
        "\n",
        "The data is ready to train a machine learning model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Function\n",
        "\n",
        "Define a function that can be used to create a model pipeline that can be trained and then used for scoring. This pipeline has 2 steps: preprocessing and model training.\n",
        "\n",
        "<b>Preprocessing Stages:</b>\n",
        "The preprocessing step of the pipeline also has 2 stages, one for numerical features and one for categorical features.\n",
        "For the numerical features, let's fill in any blanks with 0's. While the training data may not have any nulls in the these fields, future data that is scored may and this step will take care of those for us. Optionally, a scaler transformation could be added in this step as well. Similarly for the categorical variables, let's have the null values filled with \"MISSING\". Additionally to the categorical variables, these will need to be one hot encoded, so we will include that step in our pipeline.\n",
        "\n",
        "<b>Model Training Stage:</b>\n",
        "An input parameter will determine which type of model of train. Let's test out a linear regression and random forest model to start. \n",
        "\n",
        "The two steps are put together into the pipeline which is what the function is returning."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def createClassModel(algo_name, catg, nums):\n",
        "  numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value=0))])\n",
        "  \n",
        "  categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value=\"MISSING\")), ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "  \n",
        "  preprocesser = ColumnTransformer(transformers=[('num', numeric_transformer, nums), ('cat', categorical_transformer, catg)])\n",
        "  \n",
        "  if algo_name == 'linear_regression':\n",
        "    model=Ridge(alpha=100)\n",
        "  elif algo_name == 'random_forest':\n",
        "    model = RandomForestRegressor()\n",
        "  else:\n",
        "    pass\n",
        "  ModelPipeline = Pipeline(steps=[('preprocessor', preprocesser), (\"model\", model)])\n",
        "  return ModelPipeline"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the arguments that will be passed to the function. `catg_cols` is a list of the categorical variables that will be transformed in our processing step. `num_cols` is a list of the numerical variables that will be transformed in our processing step. Let's define the target column as `label` so it can be used in future steps as well."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "catg_cols = [\"vendorID\", \"month_num\", \"day_of_month\", \"normalizeHolidayName\", \"isPaidTimeOff\"]\n",
        "num_cols = [\"passengerCount\", \"tripDistance\", \"precipTime\", \"temperature\", \"precipDepth\", \"hr_sin\", \"hr_cos\", \"dy_sin\", \"dy_cos\"]\n",
        "label = [\"totalAmount\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training is ready to begin, but first, let's make sure that the categorical variables are strings in our dataframe to ensure no errors in our pipeline. \n",
        "\n",
        "Next, the data is split into training and test sets by using the `train_test_split()` function in the `scikit-learn` library. The `test_size` parameter determines the percentage of data to allocate to testing. The `random_state` parameter sets a seed to the random number generator, so that your train-test splits are deterministic.\n",
        "\n",
        "The training will happen in the for loop so that both algorithms can be tested. The createClassModel funtion is called to retreive the pipeline that can then be trained using the training dataset. \n",
        "\n",
        "Once trained, the test dataset is then ran through the model to test the model's performance. Using various functions from sklearn.metrics, the R2 score, MAPE, and RMSE can be used to measure model performance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure categorical columns are strings\n",
        "final_df[catg_cols] = final_df[catg_cols].astype(\"str\")\n",
        "\n",
        "# split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_df.drop(label, axis=1), final_df[label], test_size=0.2, random_state=222)\n",
        "\n",
        "# test 2 algorithms\n",
        "for algorithmname in [\"linear_regression\", 'random_forest']:\n",
        "    fitPipeline = createClassModel(algorithmname, catg_cols, num_cols) # get pipeline\n",
        "    fitPipeline.fit(X_train, y_train.values.ravel())                   # fit pipeine\n",
        "\n",
        "    y_pred = fitPipeline.predict(X_test)                               # score with fitted pipeline\n",
        "\n",
        "    # Evaluate\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "    print(algorithmname)\n",
        "    print(\"R2:\", r2)\n",
        "    print(\"MAPE:\", mape)\n",
        "    print(\"RMSE:\", rmse)\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "74e9702761b8f12846716a18132904990016d49f378e22e0e13a0e91318de754"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}